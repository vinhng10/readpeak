{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will work with a sample dataset containing ad impression and click data.\n",
    "\n",
    "The primary objective of this assignment is to analyze the data, derive meaningful insights, and build predictive models based on the patterns you uncover, to ultimatly improve click prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided has been undersampled to ensure that clicks represent 5% of the total data, as opposed to the original 0.4%. It consists of one week of advertising data from Finland where one row represents an ad impression (view). The data includes the following features:\n",
    "\n",
    "- Label: A binary feature indicating whether the ad was clicked (1) or not (0).\n",
    "- art: The ad ID.\n",
    "- loc: The site ID, representing the website where the ad appeared.\n",
    "- tag: The placement ID assigned by the site to indicate the ad slot on the site where the ad was shown.\n",
    "- dt: The device type on which the ad was displayed (mobile, tablet, or desktop).\n",
    "- type: The type of advertisement, either banner or native.\n",
    "- os: The operating system of the device.\n",
    "- lt: The local time when the ad was displayed.\n",
    "- make: The make (manufacturer) of the device.\n",
    "- client: The client ID representing the advertiser.\n",
    "- lang: The language of the browser.\n",
    "- cl: The number of clicks the ad has received.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data & exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!pip install pycaret -q\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "import category_encoders as ce\n",
    "from utils import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYCARET_CUSTOM_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "\n",
    "N = 10\n",
    "\n",
    "# Load dataset:\n",
    "df = pd.read_feather(\"readpeak_data.feather\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What potential challenges can you identify from the given data? and give a brief explanation how you would address them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first remove all duplicate data. We can see that there are only 113 duplicated rows, which is very small compared to the total rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicates:\n",
    "print(f\"Number of duplicates: {df.duplicated().sum()} / {len(df)}\")\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next some light preprocessing:\n",
    "\n",
    "- Convert datetime column **lt** to datetime type.\n",
    "- For string type columns, lowercase and strip whitespace for them.\n",
    "- Ensure **tag** id is unique across **loc** site, because some sites assign same tag id for total different tags. Probably because they are not aware of each others systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Ensure correct data types:\n",
    "df[\"lt\"] = pd.to_datetime(df[\"lt\"].map(lambda t: t.split(\".\")[0]))\n",
    "\n",
    "# Lowercase and strip whitespace all string columns:\n",
    "df = df.applymap(lambda s: s.lower().strip() if type(s) == str else s)\n",
    "\n",
    "# Ensure \"tag\" is unique across \"loc\":\n",
    "df[\"_tag\"] = df[\"tag\"]\n",
    "df[\"tag\"] = df[\"tag\"] + \"-\" + df[\"loc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then normalize some of the columns. In particular:\n",
    "\n",
    "- **os**: Similar os type such as \"apple ios\", \"ipados\", \"ios\" will be normalized to just \"ios\".\n",
    "- **lang**: Language \"en-us\" or \"sv-se\" should be normalized to \"en\" or \"sv\" respectively.\n",
    "- **make**: \"samsungtablet\" or \"huaweitablet\" will have \"tablet\" removed because this information is in **dt** column. Value like \"iphone\", \"mac\", etc will be normalized to \"apple\" because they are all manufactored by \"apple\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Normalize \"os\", \"lang\", and \"make\":\n",
    "df[\"os\"] = df[\"os\"].apply(normalize_os)\n",
    "df[\"lang\"] = df[\"lang\"].apply(normalize_lang)\n",
    "df[\"make\"] = df[\"make\"].apply(normalize_make)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will handle missing data. We first plot the percentage of missing values and value frequency distribution for each columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot percentage of missing data:\n",
    "missings = (\n",
    "    (df.drop(\"_tag\", axis=1).isnull().mean() * 100)\n",
    "    .round(2)\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n",
    "ax[0].set_title(\"Percentage of Missing Data\")\n",
    "ax[0].set_ylabel(\"%\")\n",
    "sns.barplot(x=missings.index, y=missings.values, ax=ax[0])\n",
    "\n",
    "\n",
    "# Function to calculate percentages of categories with counts less than 10\n",
    "def calculate_percentage(column):\n",
    "    value_counts = column.value_counts()\n",
    "    count_less_than_10 = 100 * len(value_counts[value_counts <= 10]) / len(value_counts)\n",
    "    return count_less_than_10\n",
    "\n",
    "\n",
    "percentage_df = (\n",
    "    df.drop(\"_tag\", axis=1)\n",
    "    .select_dtypes(include=\"object\")\n",
    "    .apply(calculate_percentage)\n",
    "    .reset_index()\n",
    ")\n",
    "percentage_df.columns = [\"column\", \"percentage\"]\n",
    "percentage_df = percentage_df.sort_values(by=\"percentage\", ascending=False)\n",
    "sns.barplot(x=\"column\", y=\"percentage\", data=percentage_df, ax=ax[1])\n",
    "ax[1].set_title(\"Percentage of Categories with <= 10 Impression\")\n",
    "ax[1].set_ylabel(\"%\")\n",
    "plt.show()\n",
    "\n",
    "# Create tab widget\n",
    "tab = widgets.Tab()\n",
    "contents = []\n",
    "\n",
    "columns = list(missings.loc[missings > 0].index)\n",
    "for column in columns:\n",
    "    # Create an output widget for each plot\n",
    "    output = widgets.Output()\n",
    "\n",
    "    with output:\n",
    "        fig, ax = plt.subplots(figsize=(10, 3))\n",
    "        sns.countplot(\n",
    "            data=df,\n",
    "            x=column,\n",
    "            order=df[column].value_counts().nlargest(50).index,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "        ax.set_title(f\"{column.capitalize()} Total Impressions\", fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    # Append the output widget to the outputs list\n",
    "    contents.append(output)\n",
    "\n",
    "# Set the tab children to the list of outputs\n",
    "tab.children = contents\n",
    "tab.titles = columns\n",
    "\n",
    "# Display the tab widget\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we can see that:\n",
    "\n",
    "- Almost 60% of **\"lang\"** are missing, and it's heavily skewed to **fi**. The data are collected in Finland, so it's very likely that the missing values are mostly **fi**. There is also no information about the language of the ads, so imputing those missing values probably won't help much for the prediction -> Drop **lang** column.\n",
    "- **\"make\"** also has large missing percentage. I believe that **os** and **dt** are closely related to **make**. Chi-Square Test for Independence can be used to check it -> Drop **make** column.\n",
    "- **\"os\"**, **\"city\"**, **\"client\"**, **\"tag\"** missing values will be imputed with **unknown**.\n",
    "- Lots of categories in each column have a minimal number of impressions. One option is replacing those rare categories with **others**, which might affect the analysis. This will be taken care of later when we deal with categorical encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Drop \"lang\" column:\n",
    "df = df.drop([\"lang\", \"make\"], axis=1)\n",
    "\n",
    "# Impute missing values with \"unknown\":\n",
    "df = df.fillna(\"unknown\")\n",
    "df[\"tag\"] = df[\"_tag\"] + \"-\" + df[\"loc\"]\n",
    "df = df.drop(\"_tag\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reset index:\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Keep a copy of processed dataframe:\n",
    "_df = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Total Impressions vs CTR\n",
    "\n",
    "Let's first understand the relationship between the number of impressions of each category and Click Through Rate (% CTR). From the visualization we can see that:\n",
    "\n",
    "- CTRs of categories of **art**, **loc**, **tag**, **client** vary significantly. Some of them even go up to 20%, which might indicate that they are:\n",
    "  - **loc**: site that is very good at showing ads\n",
    "  - **tag**: page that goes very well with ads\n",
    "  - **client**: client that has several successful campaigns\n",
    "  - **art**: ads that is very attractive and performant\n",
    "- CTRs of **dt**, **type**, **os**, **city** doesn't seem to vary much. Those of **city** are even plateau. This might indicate that these columns are not strong predictors for CTR.\n",
    "\n",
    "**Note**: I believe the given data contains only impression that Readpeak bidded successfully. This is not the actually the total impressions that hit Readpeak's system. Won impressions can still be a good proxy of the total incoming impressions though.\n",
    "\n",
    "**CTR Calculation**: Using Bayesian estimation, I calculate CTR by posterior mean of a binomial distribution with beta prior $Beta(\\alpha, \\beta)$. Given the CTR is usually around 0.4%, the formula is:\n",
    "\n",
    "$$\n",
    "CTR = 100 * \\frac{\\alpha + clicks}{\\alpha + \\beta + impressions}, where \\{^{\\alpha = 4}_{\\beta = 996}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "columns = list(df.drop([\"lt\", \"label\", \"cl\"], axis=1).columns)\n",
    "\n",
    "# Create tab widget\n",
    "tab = widgets.Tab()\n",
    "contents = []\n",
    "\n",
    "for i, column in enumerate(columns):\n",
    "    fig = plot_category_vs_ctr(df, column, 50)\n",
    "    output = widgets.Output()\n",
    "    with output:\n",
    "        display(fig)\n",
    "    contents.append(output)\n",
    "\n",
    "# Set the tab children to the list of outputs\n",
    "tab.children = contents\n",
    "tab.titles = columns\n",
    "\n",
    "# Display the tab widget\n",
    "display(widgets.VBox([widgets.HTML(value=\"<h1>Total Impressions vs CTR</h1>\"), tab]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTICE: The next code block might take a few minutes to finish the visualization. This could be improved by making it renders more lazily.\n",
    "\n",
    "#### 1.2.2 Ads Performance\n",
    "\n",
    "Next let's visualize how and ad's total impressions and performance CTR change over time. Each **art** tab is an ad performance across all **tag**, and also the breakdown of performance per tag id, overall **tag** overall performance and **loc** overall performance. Here are some observations:\n",
    "\n",
    "- Ads tends to have higher CTR in **tag** placements which have higher impression and CTR\n",
    "- Some ads have high CTR, but their CTRs decrease over time. This might indicate that people are less likely to click on the same ads displayed in the same place if they have seen it before.\n",
    "- Some clear seasonal trends in the amount of impressions and CTR from the site and placement. There are more impressions at certain hours than others, especially from 6AM to 12AM. Maybe these sites are newspapers.\n",
    "- Whether a day is weekday or weekend doesn't seem to affect the amount of impressions and CTR.\n",
    "- Some pairs seem to perform consistently whenever the ads are displayed in the same place.\n",
    "\n",
    "From there, I believe that hour of the day, hourly/cumulative impressions/CTR in that hour can be additional features because they inform about the amount of traffic a site or placement receives at a particular time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "# Create a tab widget\n",
    "tab = widgets.Tab()\n",
    "tab_contents = []\n",
    "\n",
    "# # Loop through each 'loc' id and create a tab for each one\n",
    "art_ids = select_for_plot(df, \"art\", N)\n",
    "for art_id in art_ids:\n",
    "    art_content = create_content(df, art_id)\n",
    "\n",
    "    # Create a 'tag' tab for each art id\n",
    "    tag_tab = widgets.Tab()\n",
    "    tag_contents = []\n",
    "\n",
    "    tag_ids = select_for_plot(filter_df(df, {\"art\": art_id}), \"tag\", N)\n",
    "    for tag_id in tag_ids:\n",
    "        tag_content = create_content(df, art_id, tag_id)\n",
    "        tag_contents.append(tag_content)\n",
    "    tag_tab.children = tag_contents\n",
    "    tag_tab.titles = tag_ids\n",
    "\n",
    "    # Add the layout to the tab contents\n",
    "    tab_contents.append(\n",
    "        widgets.VBox(\n",
    "            [\n",
    "                art_content,\n",
    "                widgets.HTML(value=\"<h2>Ad Performance Per Tag</h2>\"),\n",
    "                tag_tab,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# # Set up the tabs in the widget\n",
    "tab.children = tab_contents\n",
    "tab.titles = art_ids\n",
    "\n",
    "# Display the tab widget\n",
    "display(widgets.VBox([widgets.HTML(value=\"<h1>Overall Ad Performance</h1>\"), tab]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create at least two new features from the existing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "df = _df.copy(deep=True)\n",
    "\n",
    "# Create \"Hour\" feature:\n",
    "df[\"hour\"] = df[\"lt\"].dt.hour\n",
    "\n",
    "# Create \"Art-Tag Cumulative CTR\" feature:\n",
    "df[\"art_tag\"] = df[\"art\"] + \"-\" + df[\"tag\"]\n",
    "df = df.sort_values(by=[\"art_tag\", \"lt\"])\n",
    "df[\"art_tag_cumu_ctr\"] = posterior_ctr(\n",
    "    df.groupby(\"art_tag\")[\"label\"].cumsum().shift(1, fill_value=0),\n",
    "    (df.groupby(\"art_tag\").cumcount() + 1).shift(1, fill_value=0),\n",
    ")\n",
    "df = df.drop(columns=[\"art_tag\"])\n",
    "\n",
    "# Create \"Tag Hourly Impression\" feature:\n",
    "df = df.sort_values([\"tag\", \"lt\"])\n",
    "df[\"tag_hourly_impressions\"] = (\n",
    "    df.groupby(\"tag\")\n",
    "    .apply(\n",
    "        lambda group: group[[\"tag\", \"lt\"]]\n",
    "        .rolling(\"1H\", on=\"lt\")\n",
    "        .count()[\"tag\"]\n",
    "        .shift(1, fill_value=0)\n",
    "    )\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Create \"Tag Hourly CTR\" feature:\n",
    "df = df.sort_values([\"tag\", \"lt\"])\n",
    "df[\"tag_hourly_clicks\"] = (\n",
    "    df.groupby(\"tag\")\n",
    "    .apply(\n",
    "        lambda group: group[[\"label\", \"lt\"]]\n",
    "        .rolling(\"1H\", on=\"lt\")\n",
    "        .sum()[\"label\"]\n",
    "        .shift(1, fill_value=0)\n",
    "    )\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "df[\"tag_hourly_ctr\"] = posterior_ctr(\n",
    "    df[\"tag_hourly_clicks\"], df[\"tag_hourly_impressions\"]\n",
    ")\n",
    "df = df.drop(\"tag_hourly_clicks\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What new features did you create and why?\n",
    "\n",
    "From the analysis, the following features are worth adding to improve click prediction:\n",
    "\n",
    "- **Hour**: CTR varies significantly by the hour of the day, making the hour of impression a useful predictor. This can be easily derived from the timestamp.\n",
    "- **Tag Hourly Impressions** and **Tag Hourly CTR**: Higher overall **tag** impressions and CTR tend to lead to higher **art** CTR. High **tag** impressions but low CTR tend to yield low **art** CTR. These features basically tell the overall value of the **tag**. Those of **loc** seem to be not suitable because they provide too coarse information.\n",
    "- **Art-Tag Cumulative CTR**: This feature tracks the cumulative performance of an **art** on a specific **tag**. Increasing **art** cumulative CTR indicates it is more likely to continue to be clicked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you expect these features to improve the performance of a machine learning model?\n",
    "\n",
    "These new features are expected to enhance the machine learning model's performance by providing richer context and capturing more detailed patterns in ad engagement:\n",
    "\n",
    "- **Hour**: Click behavior often varies by time of day. Adding the Hour feature helps the model detect temporal trends, improving its ability to predict clicks based on user activity at different times.\n",
    "- **Tag Hourly Impressions** and **Tag Hourly CTR**: These features reveal the effectiveness of specific ad placements over time. They allow the model to learn which slots have more impression, and are more likely to result in clicks.\n",
    "- **Art-Tag Cumulative CTR**: Tracking the cumulative performance of an ad in specific placements provides historical context, helping the model predict future clicks based on long-term trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What kind of external data could be used to improve the predictive performance of the ML model?\n",
    "\n",
    "External data that adds semantic context and enriches metadata about the site, ad placement, client, and ad content can be valuable. Here are some potential additional features:\n",
    "\n",
    "- **Site-related**: Site category (e.g., news, e-commerce), site popularity, content genre, typical user demographics.\n",
    "- **Ad placement**: Page content (headline/theme), position on page (above/below fold), page engagement (time spent, bounce rate).\n",
    "- **Client-related**: Advertiser industry, brand awareness, historical ad performance.\n",
    "- **Ad content**: Theme or product promoted, ad sentiment, visual attributes, and target audience alignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model selection and building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of the model is to predict the likelyhood of a click\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Val/Test Splits\n",
    "\n",
    "- Let's first experimenting with just 0.1% of the original data for model selection and hyperparameter tuning. After that we can split train-val (70%), test (30%) as usual for final model.\n",
    "- Also, we will try first with the original features without those added from feature engineer steps. Then we can try adding them later.\n",
    "- Dataset will be split based on time, that is, we use data at the beginning of the week to train models and predict for the rest of the week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility:\n",
    "seed = 42\n",
    "\n",
    "df = _df.sort_values(by=\"lt\").drop([\"lt\"], axis=1).drop_duplicates()\n",
    "split_index = int(len(df) * 0.001)\n",
    "\n",
    "df_train, df_test = df.iloc[:split_index], df.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Encoding\n",
    "\n",
    "In out dataset, features **art**, **loc**, **tag**, **client**, **city** have high cardinality, making **OneHotEncoder** unsuitable, because the data matrix will be very sparse and memory intensive. We will experiment with **HashingEncoder**, **LeaveOneOutEncoder**.\n",
    "\n",
    "- **HashingEncoder**: The choice of number of components follows the recommendations from [Lucas Bernardi](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087), that is, $20 * features$\n",
    "- **LeaveOneOutEncoder**: One modification I made is that instead of simply compute the mean of target as the encoding (which is also %CTR), I use the posterior estimation of CTR. The reason is that some categories is very rare but still have clicks, naive CTR would make the numerical encoding very high, which might not reflect the true characteristic of the category. Collisions will probably happen for rare categories. This is maybe desired because for the model, rare categories can be treated indifferently; if they are rare then the chance the ads get clicked is also small. This is like converting those rare categories to \"others\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    \"Hashing\": ce.HashingEncoder(n_components=160),\n",
    "    \"Leave One Out\": CTRLeaveOneOutEncoder(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "For convenient experimentation, I will utilize [Pycaret](https://pycaret.readthedocs.io/en/latest/index.html) library to conduct many experiments to compare several models' performance. This might take a couple of minutes so please wait. Set the flag **use_gpu=True** for speedup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, encoder in encoders.items():\n",
    "    print(f\"Run Pycaret With {name} Encoder\")\n",
    "    setup(\n",
    "        data=df_train,\n",
    "        target=\"label\",\n",
    "        train_size=0.5,\n",
    "        fix_imbalance=False,\n",
    "        max_encoding_ohe=0,\n",
    "        encoding_method=encoder,\n",
    "        normalize=True,\n",
    "        normalize_method=\"zscore\",\n",
    "        session_id=seed,\n",
    "        fold=5,\n",
    "        use_gpu=False,\n",
    "    )\n",
    "\n",
    "    best = compare_models(exclude=[\"knn\"], sort=\"F1\", n_select=1)\n",
    "    print(f\"Test Best Model On Hold-out Test Dataset\")\n",
    "    predict_model(best, data=df_test)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "This result is quite surprising. With leave-one-out encoding, tree-based models yield almost perfect performance. We can be skeptical that maybe this encoding strategy leak too much information. However, we use only 0.1% of data to fit the encoder, and the model still performs well on 99.9% of unseen data.\n",
    "\n",
    "Let's try with a bit more extreme case where we keep only **tag** and **art** features to see if this setup is still good. Because the amount of unique data points will significantly drop as we only use 2 features, this time we will use 10% of data for training. Thes rest will be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import *\n",
    "\n",
    "df_extreme = _df.sort_values(by=\"lt\")[[\"art\", \"tag\", \"label\"]].drop_duplicates()\n",
    "split_index = int(len(df_extreme) * 0.1)\n",
    "\n",
    "df_train, df_test = df_extreme.iloc[:split_index], df_extreme.iloc[split_index:]\n",
    "\n",
    "setup(\n",
    "    data=df_train,\n",
    "    target=\"label\",\n",
    "    train_size=0.5,\n",
    "    fix_imbalance=False,\n",
    "    max_encoding_ohe=0,\n",
    "    encoding_method=CTRLeaveOneOutEncoder(),\n",
    "    normalize=True,\n",
    "    normalize_method=\"zscore\",\n",
    "    session_id=seed,\n",
    "    fold=5,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "best = compare_models(\n",
    "    include=[\"ada\", \"gbc\", \"catboost\", \"xgboost\", \"dt\", \"lightgbm\", \"rf\", \"et\"],\n",
    "    sort=\"F1\",\n",
    "    n_select=1,\n",
    ")\n",
    "print(f\"Test Best Model On Hold-out Test Dataset\")\n",
    "_ = predict_model(best, data=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all! With only limited data we can still have pretty good result with leave-one-out encoding and tree-based models. More optimization and hyperparater tuning can be conducted to find even better model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given the advertising ecosystem, speed of prediction is crucial. How would you change the selected models with this restraint in mind? Explain your reasoning.\n",
    "\n",
    "From the experiment, I can say that I would stick with tree-based models. To make them satisfy the latency constraint, we can find their efficient implementation that leverage software or hardware acceleration to speed up inference for a single request. Then in the system level, we can scale out the bid servers to have multiple instances with enough capacity to handle incoming bid requests.\n",
    "\n",
    "We can also leverage multi-stage architecture, that instead of having one model to handle all requests, we can have some lightweight model to first make prediction. If lightweight models are confident enough, then response directly, otherwise pass to the next stage for more complex models, with some filtering for a small subset of relevant ads.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
